{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import torch\n",
    "\n",
    "# Add parent directory to path to import src modules\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "from data.audio_loader import load_audio\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "\n",
    "This notebook explores the LibriSpeech dataset to understand:\n",
    "1.  Audio waveforms and Mel-spectrograms.\n",
    "2.  Duration distribution (to determine UAP length).\n",
    "3.  Amplitude statistics (normalization verification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_sample(filepath):\n",
    "    \"\"\"Load audio using the custom audio loader.\"\"\"\n",
    "    try:\n",
    "        # Load audio with librosa to get original SR for display, \n",
    "        # but rely on custom loader for processing if needed.\n",
    "        y, sr = librosa.load(filepath, sr=None) # sr=None preserves original rate\n",
    "        return y, sr\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filepath}: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Waveforms & Mel-Spectrograms\n",
    "\n",
    "We will plot 5 random samples to see the raw audio and its frequency representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_audio_analysis(filepath, title=\"\"):\n",
    "    y, sr = load_audio_sample(filepath)\n",
    "    \n",
    "    if y is None:\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot Waveform\n",
    "    plt.subplot(2, 1, 1)\n",
    "    librosa.display.waveshow(y, sr=sr)\n",
    "    plt.title(f'Waveform: {title}')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    \n",
    "    # Plot Mel-spectrogram\n",
    "    plt.subplot(2, 1, 2)\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    librosa.display.specshow(mel_spec_db, sr=sr, x_axis='time', y_axis='mel', fmax=8000)\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(f'Mel-Spectrogram: {title}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Duration: {librosa.get_duration(y=y, sr=sr):.2f}s | SR: {sr}Hz | Min Amp: {y.min():.3f} | Max Amp: {y.max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Sample Audio\n",
    "\n",
    "*(Note: You must update `sample_path` with a valid LibriSpeech or CommonVoice file path)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "# Update this path to a real audio file for testing\n",
    "# Example: '/path/to/LibriSpeech/test-clean/116/121/116-121-0045.flac'\n",
    "sample_path = 'sample_audio.flac' \n",
    "\n",
    "if os.path.exists(sample_path):\n",
    "    plot_audio_analysis(sample_path, \"Sample Audio\")\n",
    "else:\n",
    "    print(\"[INFO] Sample audio file not found. Skipping visualization.\")\n",
    "    print(\"[INFO] Ensure you have downloaded data using src/data/download_data.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Duration Distribution Analysis\n",
    "\n",
    "We need to determine the maximum duration in our dataset to decide the UAP vector length (e.g., 30s vs max duration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_duration_distribution(audio_dir, max_samples=50):\n",
    "    durations = []\n",
    "    file_count = 0\n",
    "    \n",
    "    # Simple recursive search for .flac or .wav files\n",
    "    for root, dirs, files in os.walk(audio_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(('.flac', '.wav')):\n",
    "                if file_count >= max_samples:\n",
    "                    break\n",
    "                path = os.path.join(root, file)\n",
    "                y, sr = load_audio_sample(path)\n",
    "                if y is not None:\n",
    "                    durations.append(librosa.get_duration(y=y, sr=sr))\n",
    "                    file_count += 1\n",
    "    \n",
    "    if not durations:\n",
    "        print(\"No audio files found.\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(durations, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "    plt.xlabel('Duration (seconds)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Duration Distribution of {len(durations)} Samples')\n",
    "    plt.axvline(np.mean(durations), color='r', linestyle='dashed', linewidth=1, label=f'Mean: {np.mean(durations):.2f}s')\n",
    "    plt.axvline(np.max(durations), color='g', linestyle='dashed', linewidth=1, label=f'Max: {np.max(durations):.2f}s')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Total samples analyzed: {len(durations)}\")\n",
    "    print(f\"Min Duration: {min(durations):.2f}s\")\n",
    "    print(f\"Max Duration: {max(durations):.2f}s\")\n",
    "    print(f\"Mean Duration: {np.mean(durations):.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION: Update this path to your LibriSpeech directory\n",
    "audio_directory = 'data/LibriSpeech/test-clean'\n",
    "\n",
    "if os.path.exists(audio_directory):\n",
    "    analyze_duration_distribution(audio_directory)\n",
    "else:\n",
    "    print(\"[INFO] Directory not found. Cannot analyze duration distribution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amplitude Statistics\n",
    "\n",
    "Verify that the audio is normalized to [-1, 1] and check for clipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_amplitude_stats(filepath):\n",
    "    y, sr = load_audio_sample(filepath)\n",
    "    if y is None:\n",
    "        return\n",
    "    \n",
    "    stats = {\n",
    "        'min': y.min(),\n",
    "        'max': y.max(),\n",
    "        'mean': y.mean(),\n",
    "        'std': y.std(),\n",
    "        'clipped_count': np.sum(np.abs(y) > 1.0),\n",
    "        'total_samples': len(y)\n",
    "    }\n",
    "    \n",
    "    print(\"--- Amplitude Statistics ---\")\n",
    "    for k, v in stats.items():\n",
    "        print(f\"{k.replace('_', ' ').title()}: {v}\")\n",
    "    \n",
    "    if stats['clipped_count'] > 0:\n",
    "        print(f\"[WARNING] {stats['clipped_count']} samples out of {stats['total_samples']} exceed the [-1, 1] range.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(sample_path):\n",
    "    check_amplitude_stats(sample_path)\n",
    "else:\n",
    "    print(\"[INFO] Sample path not found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
