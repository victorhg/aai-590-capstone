{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "450e3971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "from src.models.whisper_wrapper import WhisperASRWithAttack\n",
    "from src.attacks.pgd import PGDAttack\n",
    "\n",
    "import src.models as models\n",
    "import src.attacks as attacks\n",
    "import src.data as data_loader\n",
    "\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3677111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2620 audio files.\n",
      "Sample: /Users/victorhugogermano/Development/soundfinal/data/LibriSpeech/test-clean/61/70970/61-70970-0040.flac\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "audio_ds = data_loader.load_dataset()\n",
    "\n",
    "audio, audio_tensor = data_loader.load_audio_tensor(audio_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c7282c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00fd00074b54818a8c6dac994248a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/capstone/lib/python3.11/site-packages/torch/functional.py:681: UserWarning: An output with one or more elements was resized since it had shape [], which does not match the required output shape [1, 3001, 201]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Resize.cpp:38.)\n",
      "  return _VF.stft(  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Transcription: ''\n"
     ]
    }
   ],
   "source": [
    "# 2. Initialize Model & Processor\n",
    "wrapper = WhisperASRWithAttack(device=device)\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "\n",
    "def decode_output(logits):\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    return processor.batch_decode(pred_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "# Baseline transcription\n",
    "with torch.no_grad():\n",
    "    # Use the original Whisper transcribe method\n",
    "    result = wrapper.model.transcribe(\n",
    "        audio_tensor.cpu().numpy().squeeze(),\n",
    "        language='en',\n",
    "        fp16=False\n",
    "    )\n",
    "    transcription_clean = result['text'].strip()\n",
    "    \n",
    "print(f\"Original Transcription: '{transcription_clean}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf30e476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing Whisper Feature Extractor Dimensions ---\n",
      "feature_extractor.mel_filters shape: (201, 80)\n",
      "Audio Tensor Shape: torch.Size([1, 480000])\n",
      "STFT Magnitudes Shape (center=True): torch.Size([1, 201, 3001])\n",
      "Magnitudes after slicing :-1: torch.Size([1, 201, 3000])\n",
      "Look at matmul: (1, 3000, 201) @ torch.Size([201, 80])\n",
      "Matmul Successful!\n",
      "Mels Shape: torch.Size([1, 80, 3000])\n",
      "\n",
      "--- Comparing with HF execute ---\n",
      "HF Output Shape: torch.Size([1, 80, 3000])\n",
      "\n",
      "--- Value Comparison ---\n",
      "Manual Mean: 1.1119, Max: 1.4122, Min: -0.0208\n",
      "HF Mean: 1.1119, Max: 1.4122, Min: -0.0208\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "def test_whisper_dimensions():\n",
    "    print(\"--- Testing Whisper Feature Extractor Dimensions ---\")\n",
    "    model_path = \"openai/whisper-base\"\n",
    "    try:\n",
    "        feature_extractor = WhisperFeatureExtractor.from_pretrained(model_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading feature extractor: {e}\")\n",
    "        return\n",
    "\n",
    "    mel_filters = feature_extractor.mel_filters\n",
    "    print(f\"feature_extractor.mel_filters shape: {mel_filters.shape}\")\n",
    "    \n",
    "    # Expected: (80, 201) usually (n_mels, n_freq)\n",
    "    \n",
    "    # Simulate Audio\n",
    "    sr = 16000\n",
    "    seconds = 30\n",
    "    audio = np.random.randn(sr * seconds).astype(np.float32)\n",
    "    audio_tensor = torch.from_numpy(audio).unsqueeze(0)\n",
    "    \n",
    "    print(f\"Audio Tensor Shape: {audio_tensor.shape}\")\n",
    "    \n",
    "    # Manual STFT replication attempt (from wrapper code)\n",
    "    n_fft = 400\n",
    "    hop_length = 160\n",
    "    window = torch.hann_window(n_fft)\n",
    "    \n",
    "    # 1. Pad/Crop to 30s\n",
    "    if audio_tensor.shape[1] < 480000:\n",
    "         audio_tensor = torch.nn.functional.pad(audio_tensor, (0, 480000 - audio_tensor.shape[1]))\n",
    "    else:\n",
    "         audio_tensor = audio_tensor[:, :480000]\n",
    "\n",
    "    stft = torch.stft(\n",
    "        audio_tensor,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        window=window,\n",
    "        center=True,\n",
    "        return_complex=True\n",
    "    )\n",
    "    magnitudes = stft.abs() ** 2\n",
    "    # magnitudes shape: (Batch, Freq, Time) = (1, 201, 3001) usually for center=True 480000 samples\n",
    "    \n",
    "    print(f\"STFT Magnitudes Shape (center=True): {magnitudes.shape}\")\n",
    "    \n",
    "    # Wrapper code does magnitudes[:, :, :-1]\n",
    "    magnitudes = magnitudes[:, :, :-1]\n",
    "    print(f\"Magnitudes after slicing :-1: {magnitudes.shape}\") # Should be (1, 201, 3000)\n",
    "\n",
    "    # Convert mel_filters to tensor\n",
    "    mel_filters_tensor = torch.from_numpy(mel_filters).float()\n",
    "    \n",
    "    # Wrapper Logic:\n",
    "    # mels = torch.matmul(magnitudes.transpose(1, 2), self.mel_filters).transpose(1, 2)\n",
    "    # magnitudes.transpose(1, 2) -> (1, 3000, 201)\n",
    "    \n",
    "    print(f\"Look at matmul: (1, 3000, 201) @ {mel_filters_tensor.shape}\")\n",
    "    \n",
    "    try:\n",
    "        mels = torch.matmul(magnitudes.transpose(1, 2), mel_filters_tensor).transpose(1, 2)\n",
    "        print(\"Matmul Successful!\")\n",
    "        print(f\"Mels Shape: {mels.shape}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Matmul Failed: {e}\")\n",
    "        print(\"Trying with Transpose of filters...\")\n",
    "        try:\n",
    "             mels = torch.matmul(magnitudes.transpose(1, 2), mel_filters_tensor.T).transpose(1, 2)\n",
    "             print(\"Matmul with .T Successful!\")\n",
    "             print(f\"Mels Shape: {mels.shape}\")\n",
    "        except RuntimeError as e2:\n",
    "             print(f\"Matmul with .T Failed: {e2}\")\n",
    "\n",
    "    # Log Logic Check\n",
    "    # The feature extractor output\n",
    "    print(\"\\n--- Comparing with HF execute ---\")\n",
    "    hf_out = feature_extractor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    hf_mels = hf_out.input_features\n",
    "    print(f\"HF Output Shape: {hf_mels.shape}\")\n",
    "    \n",
    "    if 'mels' in locals():\n",
    "        # Complete the manual process to compare values\n",
    "        log_mels = torch.log10(torch.clamp(mels, min=1e-10))\n",
    "        log_mels = torch.maximum(log_mels, log_mels.max() - 8.0)\n",
    "        log_mels = (log_mels + 4.0) / 4.0\n",
    "        \n",
    "        print(\"\\n--- Value Comparison ---\")\n",
    "        print(f\"Manual Mean: {log_mels.mean().item():.4f}, Max: {log_mels.max().item():.4f}, Min: {log_mels.min().item():.4f}\")\n",
    "        print(f\"HF Mean: {hf_mels.mean().item():.4f}, Max: {hf_mels.max().item():.4f}, Min: {hf_mels.min().item():.4f}\")\n",
    "        \n",
    "        # Check if identical (unlikely to be exactly identical due to float/implementation diffs, but should be close)\n",
    "        # Note: HF implementation padding logic is slightly different (reflect vs constant, center=False)\n",
    "        # HF does:\n",
    "        # waveform = np.pad(waveform, ...)\n",
    "        # window = np.hanning(n_fft)\n",
    "        # stft = np.librosa.stft(..., center=True, pad_mode=\"reflect\") <--- WAIT, HF uses center=True?\n",
    "        \n",
    "        # Actually HF implementation details:\n",
    "        # self.feature_extractor(raw_speech) calls `_compute_log_mel_spectrogram`\n",
    "        # which calls `stft(..., center=True)` ?\n",
    "        \n",
    "        # feature_extractor class says:\n",
    "        # padding_side = \"right\"\n",
    "        # padding_value = 0.0\n",
    "        \n",
    "        pass\n",
    "\n",
    "\n",
    "test_whisper_dimensions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b32026a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mel Filters Shape: torch.Size([201, 80])\n",
      "Device: mps:0\n",
      "Audio Tensor Shape: torch.Size([66640])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mel Filters Shape: {wrapper.mel_filters.shape}\")\n",
    "print(f\"Device: {wrapper.mel_filters.device}\")\n",
    "\n",
    "# Also check audio tensor shape\n",
    "print(f\"Audio Tensor Shape: {audio_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9e8106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Perform PGD Attack\n",
    "# Epsilon 0.02 is approx -34dB relative to max amplitude 1.0 (roughly)\n",
    "attacker = PGDAttack(wrapper, epsilon=0.02, alpha=0.002, num_iter=30) \n",
    "\n",
    "print(\"Running PGD...\")\n",
    "adv_audio = attacker.generate(audio)\n",
    "\n",
    "# 4. Evaluate\n",
    "from src.attacks.pgd import compute_snr\n",
    "snr = compute_snr(audio.cpu().numpy(), adv_audio.cpu().numpy())\n",
    "\n",
    "with torch.no_grad():\n",
    "    res_adv = wrapper(adv_audio)\n",
    "    transcription_adv = decode_output(res_adv.logits)\n",
    "\n",
    "print(f\"Adversarial Transcription: '{transcription_adv}'\")\n",
    "print(f\"SNR: {snr:.2f} dB\")\n",
    "\n",
    "# 5. Play Audio (Optional)\n",
    "from IPython.display import Audio, display\n",
    "print(\"Adversarial Audio:\")\n",
    "display(Audio(adv_audio.cpu().numpy(), rate=16000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
