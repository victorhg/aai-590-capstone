{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal Adversarial Perturbation (UAP) Training\n",
    "\n",
    "This notebook implements the training loop for Universal Adversarial Perturbations (UAPs). The goal is to find a single perturbation vector $v$ that, when added to any input audio $x$, reduces the model's accuracy.\n",
    "\n",
    "### Strategy: Gradient Accumulation\n",
    "We iterate through the training set. For each sample $x_i$, we compute the gradient of the loss $\\mathcal{L}$ w.r.t the input $x_i$ ($\\nabla_{x_i} \\mathcal{L}$). We then project these gradients onto the current global perturbation vector $v$. This is based on the 'accumulated gradient' approach used in many UAP papers.\n",
    "\n",
    "### Key Constraints\n",
    "- **Input Length**: Whisper requires 16kHz audio. We will define a fixed UAP length (e.g., 5 seconds or 10 seconds).\n",
    "- **Clipping**: We must clip $v$ to ensure it stays within $[-1, 1]$ (or the model's input range).\n",
    "- **Gradient Flow**: Ensure $x$ has `requires_grad=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from transformers import WhisperModel\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import get_librispeech_files, load_audio, download_librispeech_sample\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "# 1. Prepare Data\n",
    "_ = download_librispeech_sample() # Ensure data exists\n",
    "all_files = get_librispeech_files()\n",
    "\n",
    "if not all_files:\n",
    "    raise RuntimeError(\"No files found!\")\n",
    "\n",
    "# Simple Dataset Class for Training\n",
    "class UAPDataset(Dataset):\n",
    "    def __init__(self, files, max_duration=10.0, sample_rate=16000):\n",
    "        self.files = files\n",
    "        self.max_duration = max_duration\n",
    "        self.sample_rate = sample_rate\n",
    "        self.max_samples = int(max_duration * sample_rate)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load and pad/crop\n",
    "        # Note: In real training, we might use the optimized DataLoader logic\n",
    "        # For prototype, on-the-fly loading is fine.\n",
    "        path = self.files[idx]\n",
    "        audio = load_audio(path, target_sr=self.sample_rate)\n",
    "        \n",
    "        # Pad or Crop to fixed length for UAP\n",
    "        if audio.shape[0] < self.max_samples:\n",
    "            padding = self.max_samples - audio.shape[0]\n",
    "            audio = torch.nn.functional.pad(audio, (0, padding))\n",
    "        else:\n",
    "            audio = audio[:self.max_samples]\n",
    "            \n",
    "        return audio\n",
    "\n",
    "# Create Dataset (Train Split)\n",
    "random.seed(42)\n",
    "random.shuffle(all_files)\n",
    "train_files = all_files[:20] # Small subset for prototype training\n",
    "\n",
    "dataset = UAPDataset(train_files)\n",
    "print(f\"Created Training Set with {len(dataset)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.whisper_wrapper import WhisperASRWithAttack\n",
    "\n",
    "# Initialize the model using our wrapper that supports gradient flow\n",
    "model = WhisperASRWithAttack(device=device)\n",
    "print(\"Model loaded with differentiable Mel-Spectrogram layer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_uap(\n",
    "    model, \n",
    "    dataset, \n",
    "    uap_length_sec=5.0, \n",
    "    epsilon=0.05,\n",
    "    lr=0.01,\n",
    "    epochs=10,\n",
    "    device=device\n",
    "):\n",
    "    \"\"\"\n",
    "    Training loop for Universal Perturbation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Initialize Global Perturbation v\n",
    "    uap_length = int(uap_length_sec * 16000)\n",
    "    print(f\"UAP Length: {uap_length} samples ({uap_length_sec}s)\")\n",
    "    \n",
    "    # v is a tensor of shape (1, uap_length) initialized with zeros\n",
    "    # We repeat it to match dataset sample lengths if needed later\n",
    "    v = torch.zeros(1, uap_length, device=device)\n",
    "    \n",
    "    # Optimizer for v\n",
    "    optimizer = torch.optim.SGD([v], lr=lr)\n",
    "    \n",
    "    # Loss function to minimize (we want to minimize the *negative* of the attack success? No, we maximize model loss)\n",
    "    # In this gradient accumulation method, we treat the accumulation of gradients as the update direction.\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n--- Epoch {epoch + 1}/{epochs} ---\")\n",
    "        \n",
    "        grad_accum = torch.zeros_like(v)\n",
    "        \n",
    "        for audio, label in tqdm(dataset, desc=\"Processing samples\"):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 2. Construct Adversarial Input\n",
    "            # Pad/Truncate audio to match UAP length\n",
    "            if audio.size(0) < uap_length:\n",
    "                pad_len = uap_length - audio.size(0)\n",
    "                audio_padded = torch.nn.functional.pad(audio, (0, pad_len))\n",
    "            else:\n",
    "                audio_padded = audio[:, :uap_length]\n",
    "            \n",
    "            # Apply Perturbation: x_adv = x + v\n",
    "            # We assume v is normalized or we handle it in clipping.\n",
    "            # Ensure x is on device\n",
    "            audio_input = audio_padded.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            _, loss, input_tensor = model(audio_input)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # 3. Gradient Accumulation Strategy\n",
    "            # We want to update v to be in the direction of the gradients of the loss w.r.t input.\n",
    "            # Note: The gradient of the loss w.r.t input is the 'attack' signal.\n",
    "            # In the standard UAP paper (Nguyen et al.), they update v to align with these gradients.\n",
    "            \n",
    "            # Accumulate gradient w.r.t the perturbation itself\n",
    "            # Grad w.r.t input (input_tensor.grad) * 1 (since perturbation is 1x1)\n",
    "            grad_accum += input_tensor.grad.detach()\n",
    "            \n",
    "        # 4. Projection & Update\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Update v: v = v - lr * normalize(grad_accum)\n",
    "        # This moves v towards the directions that increase loss\n",
    "        with torch.no_grad():\n",
    "            grad_accum = grad_accum / (torch.norm(grad_accum) + 1e-8)\n",
    "            v = v - lr * grad_accum\n",
    "            \n",
    "            # Clip to [-1, 1] (and also epsilon constraint if strictly needed)\n",
    "            v = torch.clamp(v, -epsilon, epsilon)\n",
    "            \n",
    "        print(f\"Updated v magnitude: {torch.norm(v)}\")\n",
    "        \n",
    "    return v\n",
    "\n",
    "# Run Training\n",
    "uap_vector = train_uap(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    uap_length_sec=5.0,\n",
    "    epsilon=0.1,\n",
    "    lr=0.1,\n",
    "    epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained perturbation\n",
    "torch.save(uap_vector, 'universal_perturbation_v.pt')\n",
    "print(\"Universal Perturbation saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Perturbation\n",
    "Plot the generated UAP to inspect its nature (Gaussian-like or structured)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(uap_vector[0].cpu().numpy())\n",
    "plt.title(\"Universal Adversarial Perturbation (v)\")\n",
    "plt.xlabel(\"Time (Samples)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "plt.savefig('uap_visualization.png')\n",
    "print(\"Visualization saved to uap_visualization.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
