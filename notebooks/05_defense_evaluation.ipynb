{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. Comprehensive Evaluation of Universal Adversarial Perturbations\n",
    "\n",
    "This notebook performs the final evaluation:\n",
    "1.  **Load Model**: Initialize Whisper model.\n",
    "2.  **Load UAP**: Load the pre-trained perturbation vector.\n",
    "3.  **Inference**: Run Whisper on Clean vs. Adversarial audio.\n",
    "4.  **Metrics**: Calculate WER, CER, and SNR.\n",
    "5.  **Visualization**: Plot Success Rate vs. SNR trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import jiwer\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --- Configuration ---\n",
    "# Whisper Model\n",
    "model_id = \"openai/whisper-tiny\" # or base, small\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# UAP Configuration\n",
    "# Assuming the UAP is saved as a .pt file from notebook 04\n",
    "UAP_PATH = \"src/uap_vector.pt\" \n",
    "EPSILON = 0.05\n",
    "\n",
    "# Audio Config\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "print(\"Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and UAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "print(f\"Loading Whisper model {model_id}...\")\n",
    "model = whisper.load_model(model_id, device=device)\n",
    "model.eval() # Set to evaluation mode\n",
    "\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load UAP vector\n",
    "def load_uap(path):\n",
    "    if os.path.exists(path):\n",
    "        uap = torch.load(path)\n",
    "        print(f\"UAP loaded from {path}, shape: {uap.shape}\")\n",
    "        return uap\n",
    "    else:\n",
    "        print(f\"Warning: {path} not found. Creating random UAP for demo.\")\n",
    "        # Return a random vector of max duration (e.g., 30s)\n",
    "        max_dur = 30.0\n",
    "        uap_len = int(max_dur * SAMPLE_RATE)\n",
    "        uap = torch.randn(uap_len).float() * EPSILON\n",
    "        return uap\n",
    "\n",
    "uap_vector = load_uap(UAP_PATH)\n",
    "# Pad or truncate UAP to match audio length dynamically if needed, \n",
    "# but for this demo, we assume the loader handles length alignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_snr(clean_audio, adv_audio):\n",
    "    \"\"\"Calculates Signal-to-Noise Ratio (dB)\"\"\"\n",
    "    clean = clean_audio.astype(np.float32)\n",
    "    adv = adv_audio.astype(np.float32)\n",
    "    \n",
    "    # Calculate power\n",
    "    signal_power = np.mean(clean ** 2)\n",
    "    noise_power = np.mean((clean - adv) ** 2)\n",
    "    \n",
    "    if noise_power == 0:\n",
    "        return float('inf')\n",
    "    \n",
    "    snr = 10 * np.log10(signal_power / noise_power)\n",
    "    return snr\n",
    "\n",
    "def apply_uap_to_audio(waveform, uap, eps=0.05):\n",
    "    \"\"\"\n",
    "    Aligns UAP to waveform length and applies perturbation.\n",
    "    \"\"\"\n",
    "    # Ensure waveform is numpy array\n",
    "    if isinstance(waveform, torch.Tensor):\n",
    "        waveform = waveform.cpu().numpy()\n",
    "    \n",
    "    # Simple length handling: Crop or Pad\n",
    "    orig_len = len(waveform)\n",
    "    uap_len = len(uap)\n",
    "    \n",
    "    if orig_len > uap_len:\n",
    "        # Crop UAP to fit audio\n",
    "        adv_audio = waveform + uap[:orig_len] * eps\n",
    "    else:\n",
    "        # Repeat UAP\n",
    "        repeat_factor = int(np.ceil(orig_len / uap_len))\n",
    "        uap_expanded = np.tile(uap, repeat_factor)[:orig_len]\n",
    "        adv_audio = waveform + uap_expanded * eps\n",
    "    \n",
    "    # Clip to [-1, 1]\n",
    "    adv_audio = np.clip(adv_audio, -1.0, 1.0)\n",
    "    return adv_audio\n",
    "\n",
    "def run_evaluation(model, audio_array, language=\"en\"):\n",
    "    \"\"\"Run Whisper on audio and return transcription\"\"\"\n",
    "    result = model.transcribe(audio_array, language=language, fp16=False)\n",
    "    return result[\"text\"]\n",
    "\n",
    "def wer_ref(ref, hyp):\n",
    "    return jiwer.wer(ref, hyp)\n",
    "\n",
    "def cer_ref(ref, hyp):\n",
    "    return jiwer.cer(ref, hyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Dataset (Simulated or Load LibriSpeech)\n",
    "# We will use the 'test-clean' split if available, otherwise load a subset.\n",
    "print(\"Loading LibriSpeech test-clean...\")\n",
    "librispeech = load_dataset(\"librispeech_asr\", \"clean\", split=\"test\")\n",
    "librispeech = librispeech.select(range(50)) # Use first 50 for speed in notebook\n",
    "print(f\"Loaded {len(librispeech)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Loop\n",
    "results = []\n",
    "\n",
    "print(\"Starting Evaluation...\")\n",
    "for idx, item in enumerate(librispeech):\n",
    "    # Load audio\n",
    "    # Note: librispeech audio is typically numpy array already\n",
    "    clean_audio = item[\"audio\"][\"array\"]\n",
    "    clean_text = item[\"text\"]\n",
    "    \n",
    "    # Apply UAP\n",
    "    adv_audio = apply_uap_to_audio(clean_audio, uap_vector, eps=EPSILON)\n",
    "    \n",
    "    # Inference\n",
    "    try:\n",
    "        pred_clean = run_evaluation(model, clean_audio)\n",
    "        pred_adv = run_evaluation(model, adv_audio)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {idx}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Metrics\n",
    "    w_clean = wer_ref(clean_text, pred_clean)\n",
    "    c_clean = cer_ref(clean_text, pred_clean)\n",
    "    \n",
    "    w_adv = wer_ref(clean_text, pred_adv)\n",
    "    c_adv = cer_ref(clean_text, pred_adv)\n",
    "    \n",
    "    snr = calculate_snr(clean_audio, adv_audio)\n",
    "    \n",
    "    results.append({\n",
    "        \"idx\": idx,\n",
    "        \"clean_wer\": w_clean,\n",
    "        \"clean_cer\": c_clean,\n",
    "        \"adv_wer\": w_adv,\n",
    "        \"adv_cer\": c_adv,\n",
    "        \"snr\": snr\n",
    "    })\n",
    "    \n",
    "    if (idx + 1) % 10 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(librispeech)} samples.\")\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"Evaluation Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_results.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_rate = (df_results['adv_cer'] > 0.5).mean()\n",
    "print(f\"Attack Success Rate (CER > 0.5): {success_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: WER vs SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot Clean\n",
    "plt.scatter(df_results['snr'], df_results['clean_wer'], c='blue', alpha=0.5, label='Clean Audio', marker='o')\n",
    "\n",
    "# Plot Adversarial\n",
    "plt.scatter(df_results['snr'], df_results['adv_wer'], c='red', alpha=0.5, label='Adversarial Audio', marker='x')\n",
    "\n",
    "plt.title('WER vs SNR for Clean and Adversarial Audio')\n",
    "plt.xlabel('SNR (dB)')\n",
    "plt.ylabel('Word Error Rate')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 1.0) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defense Mechanism: Randomized Smoothing (Optional Evaluation)\n",
    "\n",
    "To demonstrate defense, we can test how adding random Gaussian noise affects the SNR and WER.\n",
    "\n",
    "*Note: This is a simplified demonstration. True Randomized Smoothing involves running the model multiple times with noise and aggregating predictions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gaussian_smoothing(audio, std_dev=0.01):\n",
    "    \"\"\"Add Gaussian noise\"\"\"\n",
    "    noise = np.random.normal(0, std_dev, audio.shape)\n",
    "    smoothed = audio + noise\n",
    "    smoothed = np.clip(smoothed, -1.0, 1.0)\n",
    "    return smoothed\n",
    "\n",
    "# Test on a few samples\n",
    "test_idx = 0\n",
    "sample_clean = df_results.iloc[test_idx]\n",
    "sample_audio = librispeech[test_idx][\"audio\"][\"array\"]\n",
    "\n",
    "smoothed_audio = apply_gaussian_smoothing(sample_audio, std_dev=0.01)\n",
    "snr_smoothed = calculate_snr(sample_audio, smoothed_audio)\n",
    "\n",
    "print(f\"Original SNR: {df_results.iloc[test_idx]['snr']:.2f} dB\")\n",
    "print(f\"Smoothed SNR: {snr_smoothed:.2f} dB\")\n",
    "print(f\"Clean WER: {sample_clean['clean_wer']:.2f}\")\n",
    "print(f\"Adversarial WER: {sample_clean['adv_wer']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
